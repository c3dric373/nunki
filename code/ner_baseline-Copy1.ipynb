{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET-UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aida-yago2-dataset  emerging.test\t     README.txt\r\n",
      "apw_eng_201010.tsv  emerging.test.annotated  wnut17train.conll\r\n",
      "apw_eng_201011.tsv  ner\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_path = \"../data/wnut17train.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../data/emerging.test.annotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/wnut17train.conll\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "df_test = pd.read_csv(test_path, header = None, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(dataframe):\n",
    "    labels = []\n",
    "    for i,row in df.iterrows():\n",
    "        if(row.O != 'O' and (not isinstance(row.O,float))):\n",
    "            labels.append(row.O)\n",
    "    labels = [x[2:len(x)] for x in labels]\n",
    "    labels = set(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corporation', 'creative-work', 'group', 'location', 'person', 'product'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = get_labels(df)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(path):\n",
    "    content = [x.replace(\"\\t\", \" \") for x in read_file(path)]\n",
    "    text = [x.split() for x in content]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_train = get_words(wnut_path)\n",
    "text_test = get_words(test_path)\n",
    "datasets = [text_train,text_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['&', 'O'],\n",
       " ['gt', 'O'],\n",
       " [';', 'O'],\n",
       " ['*', 'O'],\n",
       " ['The', 'O'],\n",
       " ['soldier', 'O'],\n",
       " ['was', 'O'],\n",
       " ['killed', 'O'],\n",
       " ['when', 'O'],\n",
       " ['another', 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(list_labels):\n",
    "    res = []\n",
    "    for i,x in enumerate(list_labels):\n",
    "        j = 1\n",
    "        if(i + j< len(list_labels) and x[1] == 'B' and list_labels[i  + j][1] == 'I'):\n",
    "            while(i + j < len(list_labels) and list_labels[i + j][1] == 'I'):\n",
    "                j += 1\n",
    "            res.append((x[0],x[0] + j -1 ,x[2]))\n",
    "        elif(x[1] == 'B'):\n",
    "            res.append([x[0],x[2]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_i_to_char_i(sent_label):\n",
    "    res_labels = []\n",
    "    sent = sent_label[0].split()\n",
    "    labels = sent_label[1]\n",
    "    \n",
    "    for label in labels: \n",
    "        if(len(label) == 2):\n",
    "            start_label_index = label[0]\n",
    "            end_label_index = label[0]\n",
    "            label_name = label[1]\n",
    "        elif(len(label) == 3):\n",
    "            start_label_index = label[0]\n",
    "            end_label_index = label[1]\n",
    "            label_name = label[2]\n",
    "            \n",
    "\n",
    "\n",
    "        start_char_index = sum([len(x) for x in sent[0:start_label_index]])    \n",
    "        end_char_index = start_char_index + sum([len(x) for x in sent[start_label_index:end_label_index + 1]])\n",
    "        res_labels.append((start_char_index + start_label_index,end_char_index + end_label_index,label_name))\n",
    "        \n",
    "            \n",
    "    return [\" \".join(sent), res_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spacy_format(sent_label): \n",
    "    sent = sent_label[0]\n",
    "    labels = sent_label[1]\n",
    "    res_dict = {'entities': labels}\n",
    "    return [sent, res_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "def create_entities_char_level(text):\n",
    "    res = []\n",
    "    sentence = []\n",
    "    sent_labels = []\n",
    "    for word_entity_pair in text:\n",
    "        if(len(word_entity_pair)!=0):\n",
    "            sentence.append(word_entity_pair[0])\n",
    "            sent_labels.append(word_entity_pair[1])\n",
    "        else:\n",
    "            sent_labels = [(i,x[0], x[2:len(x)]) for i,x in enumerate(sent_labels) if x != 'O' ]\n",
    "            sent_labels = squash(sent_labels)\n",
    "            res.append([\" \".join(sentence),sent_labels])\n",
    "            sent_labels = []\n",
    "            sentence = []\n",
    "        \n",
    "    res = [str_i_to_char_i(x) for x in res]\n",
    "    res = [to_spacy_format(x) for x in res]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = create_entities_char_level(text_train)\n",
    "EVAL_DATA = create_entities_char_level(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From Green Newsfeed : AHFA extends deadline for Sage Award to Nov . 5 http://tinyurl.com/24agj38',\n",
       " {'entities': [(22, 26, 'group')]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .',\n",
       " {'entities': [(100, 107, 'location')]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_DATA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .',\n",
       "  {'entities': [(100, 107, 'location')]}],\n",
       " ['& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .',\n",
       "  {'entities': [(54, 67, 'location')]}],\n",
       " ['& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .',\n",
       "  {'entities': []}],\n",
       " ['& gt ; * The four civilians killed included two children of a family whose house was hit by a separate avalanche , also on Wednesday , a police spokesman said .',\n",
       "  {'entities': []}],\n",
       " ['The bodies of the soldiers were recovered after the concerted efforts of the Avalanche Rescue Teams ( ART ) , which is equipped to work in inhospitable terrain and weather conditions .',\n",
       "  {'entities': [(77, 99, 'group'), (102, 105, 'group')]}],\n",
       " ['& gt ; * Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , Defence Spokesman Colonel Rajesh Kalia said .',\n",
       "  {'entities': [(161, 181, 'person')]}],\n",
       " ['Visuals of the avalanche site in Gurez sector .',\n",
       "  {'entities': [(33, 45, 'location')]}],\n",
       " ['( Source : ANI ) Visuals of the avalanche site in Gurez sector .',\n",
       "  {'entities': [(11, 14, 'group'), (50, 62, 'location')]}],\n",
       " ['( Source : ANI )', {'entities': [(11, 14, 'group')]}],\n",
       " ['“ Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , ” Defence Spokesman Colonel Rajesh Kalia said .',\n",
       "  {'entities': [(164, 176, 'person')]}],\n",
       " ['Watch What Else is Making News', {'entities': [(6, 30, 'creative-work')]}],\n",
       " ['“ Swift action by a few soldiers at the post and timely help from the villagers of Mahazgund ensured saving of six soldiers .',\n",
       "  {'entities': [(83, 92, 'location')]}],\n",
       " ['Unfortunately , three soldiers could not be saved whose bodies were retrieved on January 26 by the ARTs that were rushed to the post , ” he said .',\n",
       "  {'entities': [(99, 103, 'group')]}],\n",
       " [\"^ Function ^ : ^ I ^ post ^ the ^ article ' s ^ text ^ as ^ a ^ comment ^ if ^ the ^ website ^ is ^ adblocker ^ unfriendly .\",\n",
       "  {'entities': []}],\n",
       " ['[ ^ I ^ accept ^ commands !', {'entities': []}],\n",
       " ['Road and airport closure isolate Srinagar as avalanche risk remains high',\n",
       "  {'entities': [(33, 41, 'location')]}],\n",
       " ['[ IMAGE ] ( http://images.indianexpress.com/2015/05/drdo-logo-thumb.jpg?w=480 )',\n",
       "  {'entities': []}],\n",
       " ['The DRDO is working on projects to develop new technologies to predict avalanches in a much precise manner .',\n",
       "  {'entities': [(4, 8, 'group')]}],\n",
       " ['( File Photo ) The DRDO is working on projects to develop new technologies to predict avalanches in a much precise manner .',\n",
       "  {'entities': [(19, 23, 'group')]}],\n",
       " ['( File Photo )', {'entities': []}],\n",
       " ['The Defence Research Development Organisation ( DRDO ) is working on four projects to develop new technologies for more accurate prediction of avalanches , the government said on Friday .',\n",
       "  {'entities': [(4, 45, 'group'), (48, 52, 'group')]}],\n",
       " ['“ Presently , the DRDO is working on four projects to develop new technologies for more accurate prediction of avalanches , ” he said in a written response .',\n",
       "  {'entities': [(18, 22, 'group')]}],\n",
       " ['Replying to another question , Bhamre said the jawans deployed at places such as Siachen Glacier are provided with the best-quality winter clothing .',\n",
       "  {'entities': [(31, 37, 'person'), (81, 96, 'location')]}],\n",
       " ['He said the soldiers deployed in Siachen are being provided pre-fabricated insulated shelters .',\n",
       "  {'entities': [(33, 40, 'location')]}],\n",
       " ['“ In Navy , advisories on mental health are issued from time to time and stress relieving activities like yoga , art of living are conducted periodically .',\n",
       "  {'entities': []}],\n",
       " [\"^ Function ^ : ^ I ^ post ^ the ^ article ' s ^ text ^ as ^ a ^ comment ^ if ^ the ^ website ^ is ^ adblocker ^ unfriendly .\",\n",
       "  {'entities': []}],\n",
       " ['[ ^ I ^ accept ^ commands !', {'entities': []}],\n",
       " ['& gt ; Umm , where are all these alleged criticisms ?', {'entities': []}],\n",
       " ['This fucker should be sent up the fucking river because of what he did not because he likes Trump .',\n",
       "  {'entities': [(92, 97, 'person')]}],\n",
       " ['Bhangra is Punjabi .',\n",
       "  {'entities': [(0, 7, 'person'), (11, 18, 'location')]}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_DATA[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'location': 150,\n",
       "             'group': 165,\n",
       "             'person': 429,\n",
       "             'creative-work': 142,\n",
       "             'corporation': 66,\n",
       "             'product': 127})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "dict_labels = defaultdict(int)\n",
    "for x in EVAL_DATA:\n",
    "    entitites = x[1]['entities']\n",
    "    for y in entitites: \n",
    "        label = y[2]\n",
    "        dict_labels[label] += 1\n",
    "dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compounding(min_batch_size,max_batch_size,len_data):\n",
    "    factor = 1.001 \n",
    "    sizes = []\n",
    "    sizes.append(min_batch_size)\n",
    "    x = len_data - min_batch_size\n",
    "    batch_size = min_batch_size\n",
    "    while x > 0:\n",
    "        batch_size = batch_size * factor \n",
    "        rounded_bs = int(round(batch_size,0))\n",
    "        x -= rounded_bs\n",
    "        sizes.append(rounded_bs)\n",
    "    return sizes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(train_data):\n",
    "    if(len(train_data) == 1):\n",
    "        return train_data\n",
    "    new_batch = []\n",
    "    current_size = len(train_data[0][0]) + 1\n",
    "    new_batch_text = train_data[0][0] + \" \"\n",
    "    new_annotations = train_data[0][1]['entities'].copy()\n",
    "    for i,data in enumerate(train_data):\n",
    "        text =  data[0]\n",
    "        annotations = data[1]\n",
    "        if (i!=0):\n",
    "            new_batch_text += text + \" \"\n",
    "            annotations_batch = [(x[0] + current_size, x[1]+current_size, x[2]) \n",
    "                           for x in annotations['entities']]\n",
    "            new_annotations.extend(annotations_batch)\n",
    "            annotations_batch = []\n",
    "            current_size += len(text) + 1\n",
    "            \n",
    "            \n",
    "    new_batch.append(new_batch_text)\n",
    "    new_batch.append(new_annotations)\n",
    "    return new_batch\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(train_data, size):\n",
    "    batches = []\n",
    "    index = 0\n",
    "    for batch_size in size: \n",
    "        batch = create_batch(train_data[index:index+batch_size])\n",
    "        batches.append(batch)\n",
    "        index += batch_size\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batches(train_data, model_type):\n",
    "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 5, \"textcat\": 64}\n",
    "    max_batch_size = max_batch_sizes[model_type]\n",
    "    if len(train_data) < 500:\n",
    "        max_batch_size /= 2\n",
    "    batch_size = compounding(1, max_batch_size, len(train_data))\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Add ner pipe\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "model = None\n",
    "n_iter=1\n",
    "\n",
    "if model is not None:\n",
    "    nlp1 = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp1 = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "#create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp1.pipe_names:\n",
    "    print(\"Add ner pipe\")\n",
    "    ner = nlp1.create_pipe('ner')\n",
    "    nlp1.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "\n",
    "else:\n",
    "    ner = nlp1.get_pipe('ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, Trains data based on annotations \n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTime(start,end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "    return time_since_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(start,i,len_):\n",
    "    tenth = int(len_/20)\n",
    "    if(i % tenth == 0):\n",
    "        percent = int(round((i/len_*10),0))\n",
    "        time_ = getTime(start,time.time())\n",
    "        print(\"0%\" + \"=\" *percent + str(percent*10) + \"%, \" + time_, end=\"\\r\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 5881.43087047322}00:02:17.50\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from spacy.util import decaying\n",
    "import time\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp1.pipe_names if pipe != 'ner']\n",
    "dropout = decaying(0.6, 0.2, 1e-4)\n",
    "\n",
    "with nlp1.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp1.begin_training()\n",
    "    optimizer.alpha = 0.0001\n",
    "    losses = {}\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        drop = next(dropout)\n",
    "        batches  = [x[0] for x  in get_batches(TRAIN_DATA, \"ner\") if len(x) == 1]\n",
    "        start = time.time()\n",
    "        for i,data in enumerate(TRAIN_DATA):\n",
    "            log(start,i,len(TRAIN_DATA))\n",
    "            text, annotations = data\n",
    "            nlp1.update(\n",
    "                [text],  # batch of texts\n",
    "                [annotations],  # batch of annotations\n",
    "                drop=0.5,  # dropout \n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "            \n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the trained model\n",
    "for text,entities in EVAL_DATA:\n",
    "    doc = nlp1(text)\n",
    "    print(entities.values())\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-27 16:48:43 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-05-27 16:48:44 INFO: Use device: cpu\n",
      "2020-05-27 16:48:44 INFO: Loading: tokenize\n",
      "2020-05-27 16:48:44 INFO: Loading: ner\n",
      "2020-05-27 16:48:45 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"en\",processors='tokenize,ner')\n",
    "nlp_stanford = StanzaLanguage(snlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entities(entities):\n",
    "    ent = entities['entities']\n",
    "    new_ents = [x for x in ent if x[2] in ['location','person','group','creative-work'\n",
    "                                           ,'product']]\n",
    "    entities['entities'] = new_ents\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanf_to_conLL(label):\n",
    "    if (label == 'GPE' or label=='FAC'):\n",
    "        return 'location'\n",
    "    elif(label=='PERSON'):\n",
    "        return 'person'\n",
    "    elif(label=='PRODUCT'):\n",
    "        return 'product'    \n",
    "    elif(label=='ORG' or label=='NORP'):\n",
    "        return 'group'\n",
    "    elif(label=='WORK_OF_ART'):\n",
    "        return ' creative-work'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct entities\n",
      "[(100, 107, 'location')]\n",
      "['Sonmarg']\n",
      "Sonmarg\n",
      "GPE\n",
      "FN: 0 FP: 0 TP: 1\n",
      "FP_GLOBAL: 0\n",
      "FN_GLOBAL: 0\n",
      "TP_GLOBAL: 1\n"
     ]
    }
   ],
   "source": [
    "fp_global = 0 \n",
    "fn_global = 0 \n",
    "tp_global = 0 \n",
    "stanford  = True\n",
    "accepted_ents = ['GPE', 'PERSON','ORG','FAC','WORK_OF_ART','NORP','PRODUCT']\n",
    "for text,entities in EVAL_DATA[0:1]:\n",
    "    ent_as_list = list(entities.values())[0]\n",
    "    if(stanford):\n",
    "        entities = filter_entities(entities)\n",
    "        #correct_entities = [(x[0],x[1],stanf_to_conLL(x[2])) for x in ent_as_list]\n",
    "    correct_entities = ent_as_list\n",
    "    fp = 0 \n",
    "    fn = 0 \n",
    "    tp = 0 \n",
    "    print('correct entities')\n",
    "    print(correct_entities)\n",
    "    correct_text = [text[y[0]:y[1]] for y in correct_entities]\n",
    "    print(correct_text)\n",
    "    doc = nlp1(text)\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    predicted_entities = [x for x in predicted_entities if x[1] in accepted_ents]\n",
    "    correctly_predicted = 0 \n",
    "    for i,data in enumerate(predicted_entities): \n",
    "        predicted_text, predicted_label = data \n",
    "        #predicted_label = stanf_to_conLL(predicted_label)\n",
    "        print(predicted_text)\n",
    "        print(predicted_label)\n",
    "        predicted_label = stanf_to_conLL(predicted_label)\n",
    "        if(predicted_text in correct_text):\n",
    "            index_label = correct_text.index(predicted_text)\n",
    "            if(predicted_label == correct_entities[index_label][2]):\n",
    "                        tp += 1\n",
    "                        correctly_predicted += 1 \n",
    "        else: \n",
    "            fp += 1\n",
    "            \n",
    "    fn += (len(correct_text) - correctly_predicted)\n",
    "    \n",
    "    fp_global += fp\n",
    "    fn_global += fn\n",
    "    tp_global += tp \n",
    "    \n",
    "    print(\"FN: \" + str(fn) + \" FP: \" + str(fp) + \" TP: \" + str(tp))\n",
    "    print(\"FP_GLOBAL: \" + str(fp_global))  \n",
    "    print(\"FN_GLOBAL: \" + str(fn_global))  \n",
    "    print(\"TP_GLOBAL: \" + str(tp_global))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = tp_global/(tp_global + fp_global) \n",
    "recall= tp_global/ (tp_global + fn_global)\n",
    "f1_score = 2* ((precision*recall)/(precision+recall))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1 Score: \" + str(f1_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2,28,22,373,610\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pres_data = EVAL_DATA[2] + EVAL_DATA[22] + EVAL_DATA[28] + EVAL_DATA[373] + EVAL_DATA[610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .',\n",
       "  {'entities': []}],\n",
       " ['Replying to another question , Bhamre said the jawans deployed at places such as Siachen Glacier are provided with the best-quality winter clothing .',\n",
       "  {'entities': [(31, 37, 'person'), (81, 96, 'location')]}],\n",
       " ['This fucker should be sent up the fucking river because of what he did not because he likes Trump .',\n",
       "  {'entities': [(92, 97, 'person')]}],\n",
       " [\"@ Xantec . You ' re right , it is . The Hogwarts staff member delivering the letter would be expected to mention Diagon Alley ( as Dumbledore did ) . Hagrid neglected to do this by just posting the letter through the letterbox . So really it ' s an oversight on the part of Hagrid ( although obviously in reality he did take Harry to DA in person ) .\",\n",
       "  {'entities': [(2, 8, 'product'),\n",
       "    (40, 48, 'location'),\n",
       "    (113, 125, 'location'),\n",
       "    (131, 141, 'person'),\n",
       "    (150, 156, 'person'),\n",
       "    (274, 280, 'person'),\n",
       "    (325, 330, 'person'),\n",
       "    (334, 336, 'location')]}],\n",
       " ['Tango - In the section about the fictional enterprise and the fictional workings of the Star Trek universe there is a section of biographies of the Star Trek characters , including Kirk . My copy of the Making of Star Trek 1968 edition is packed away and unreachable as is any copy of the writers guide I may own .',\n",
       "  {'entities': [(88, 97, 'creative-work'),\n",
       "    (148, 157, 'creative-work'),\n",
       "    (181, 185, 'person'),\n",
       "    (203, 235, 'creative-work'),\n",
       "    (285, 302, 'creative-work')]}]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres_data = []\n",
    "pres_data.append( EVAL_DATA[2])\n",
    "pres_data.append( EVAL_DATA[22])\n",
    "pres_data.append( EVAL_DATA[28])\n",
    "pres_data.append( EVAL_DATA[373])\n",
    "pres_data.append( EVAL_DATA[610])\n",
    "pres_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-29 15:59:11 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-05-29 15:59:11 INFO: Use device: cpu\n",
      "2020-05-29 15:59:11 INFO: Loading: tokenize\n",
      "2020-05-29 15:59:11 INFO: Loading: ner\n",
      "2020-05-29 15:59:12 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 15:59:12,338 loading file /home/c3dric/.flair/models/en-ner-ontonotes-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "snlp = stanza.Pipeline(lang=\"en\",processors='tokenize,ner')\n",
    "tagger = SequenceTagger.load('ner-ontonotes')\n",
    "accepted_ents = ['GPE', 'PERSON','ORG','FAC','WORK_OF_ART','NORP','PRODUCT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .\n",
      "Correct entities:\n",
      "{'entities': []}\n",
      "Spacy Prediction:\n",
      "[]\n",
      "-----------\n",
      "Stanford Prediction:\n",
      "[]\n",
      "-----------\n",
      "Flair Prediction:\n",
      "[]\n",
      "-----------\n",
      "TagMe mentions:\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "army [13,17] lp=0.029292907565832138\n",
      "Thursday [21,29] lp=0.018874140456318855\n",
      "bodies [44,50] lp=0.0037941946648061275\n",
      "ten [54,57] lp=0.003412862541154027\n",
      "men [65,68] lp=0.004728082101792097\n",
      "who [69,72] lp=0.003441077657043934\n",
      "killed [78,84] lp=0.002261004177853465\n",
      "avalanche [91,100] lp=0.21192322671413422\n",
      "-----------\n",
      "TagMe annotations:\n",
      "Thursday -> Thursday (band) (score: 0.10039137303829193)\n",
      "who -> The Who (score: 0.17288707196712494)\n",
      "avalanche -> Avalanche (Matthew Good album) (score: 0.16760167479515076)\n",
      "-----------\n",
      "Replying to another question , Bhamre said the jawans deployed at places such as Siachen Glacier are provided with the best-quality winter clothing .\n",
      "Correct entities:\n",
      "{'entities': [(31, 37, 'person'), (81, 96, 'location')]}\n",
      "Spacy Prediction:\n",
      "[]\n",
      "-----------\n",
      "Stanford Prediction:\n",
      "[('Bhamre', 'person')]\n",
      "-----------\n",
      "Flair Prediction:\n",
      "[(31, 37, 'person')]\n",
      "-----------\n",
      "TagMe mentions:\n",
      "question [20,28] lp=0.006107843481004238\n",
      "Bhamre [31,37] lp=0.6666666865348816\n",
      "jawans [47,53] lp=0.2142857164144516\n",
      "deployed [54,62] lp=0.0015248128911480308\n",
      "Siachen Glacier [81,96] lp=1.0\n",
      "winter clothing [132,147] lp=0.07432432472705841\n",
      "-----------\n",
      "TagMe annotations:\n",
      "Bhamre -> Subhash Bhamre (score: 0.3333333432674408)\n",
      "jawans -> Private (rank) (score: 0.21570643782615662)\n",
      "deployed -> Military deployment (score: 0.1980465203523636)\n",
      "Siachen Glacier -> Siachen conflict (score: 0.7286824584007263)\n",
      "-----------\n",
      "This fucker should be sent up the fucking river because of what he did not because he likes Trump .\n",
      "Correct entities:\n",
      "{'entities': [(92, 97, 'person')]}\n",
      "Spacy Prediction:\n",
      "[]\n",
      "-----------\n",
      "Stanford Prediction:\n",
      "[('Trump', 'person')]\n",
      "-----------\n",
      "Flair Prediction:\n",
      "[(92, 97, 'person')]\n",
      "-----------\n",
      "TagMe mentions:\n",
      "fucker [5,11] lp=0.009661835618317127\n",
      "up [27,29] lp=0.0013194273924455047\n",
      "fucking [34,41] lp=0.009405441582202911\n",
      "river [42,47] lp=0.030226554721593857\n",
      "likes [86,91] lp=0.0019302224973216653\n",
      "Trump [92,97] lp=0.07732106745243073\n",
      "-----------\n",
      "TagMe annotations:\n",
      "fucker -> Fuck (score: 0.1464240849018097)\n",
      "-----------\n",
      "@ Xantec . You ' re right , it is . The Hogwarts staff member delivering the letter would be expected to mention Diagon Alley ( as Dumbledore did ) . Hagrid neglected to do this by just posting the letter through the letterbox . So really it ' s an oversight on the part of Hagrid ( although obviously in reality he did take Harry to DA in person ) .\n",
      "Correct entities:\n",
      "{'entities': [(2, 8, 'product'), (40, 48, 'location'), (113, 125, 'location'), (131, 141, 'person'), (150, 156, 'person'), (274, 280, 'person'), (325, 330, 'person'), (334, 336, 'location')]}\n",
      "Spacy Prediction:\n",
      "[]\n",
      "-----------\n",
      "Stanford Prediction:\n",
      "[('Xantec', 'group'), ('Hogwarts', 'group'), ('Diagon Alley', 'person'), ('Dumbledore', 'person'), ('Hagrid', 'person'), ('Hagrid', 'person'), ('Harry', 'person')]\n",
      "-----------\n",
      "Flair Prediction:\n",
      "[(2, 8, 'group'), (40, 48, 'group'), (113, 125, 'person'), (131, 141, 'person'), (150, 156, 'person'), (274, 280, 'person'), (325, 330, 'person')]\n",
      "-----------\n",
      "TagMe mentions:\n",
      "You [11,14] lp=0.003574441419914365\n",
      "right [20,25] lp=0.00291381380520761\n",
      "Hogwarts staff [40,54] lp=1.0\n",
      "member [55,61] lp=0.0037622840609401464\n",
      "Diagon Alley [113,125] lp=0.05999999865889549\n",
      "Dumbledore [131,141] lp=0.290076345205307\n",
      "Hagrid [150,156] lp=0.465753436088562\n",
      "neglected [157,166] lp=0.0012674271129071712\n",
      "letterbox [217,226] lp=0.5477706789970398\n",
      "oversight [249,258] lp=0.006272920407354832\n",
      "obviously [292,301] lp=0.0016255356604233384\n",
      "reality [305,312] lp=0.022829324007034302\n",
      "Harry [325,330] lp=0.007404623553156853\n",
      "DA [334,336] lp=0.008053525350987911\n",
      "in person [337,346] lp=0.006759611424058676\n",
      "-----------\n",
      "TagMe annotations:\n",
      "right -> Ethics (score: 0.14588184654712677)\n",
      "Hogwarts staff -> Hogwarts staff (score: 0.5)\n",
      "Diagon Alley -> The Wizarding World of Harry Potter (Universal Orlando Resort) (score: 0.2633545994758606)\n",
      "Dumbledore -> Albus Dumbledore (score: 0.38127678632736206)\n",
      "Hagrid -> Rubeus Hagrid (score: 0.46911531686782837)\n",
      "letterbox -> Letterboxing (filming) (score: 0.2738853394985199)\n",
      "Hagrid -> Rubeus Hagrid (score: 0.232876718044281)\n",
      "reality -> Reality television (score: 0.1681847721338272)\n",
      "-----------\n",
      "Tango - In the section about the fictional enterprise and the fictional workings of the Star Trek universe there is a section of biographies of the Star Trek characters , including Kirk . My copy of the Making of Star Trek 1968 edition is packed away and unreachable as is any copy of the writers guide I may own .\n",
      "Correct entities:\n",
      "{'entities': [(88, 97, 'creative-work'), (148, 157, 'creative-work'), (181, 185, 'person'), (203, 235, 'creative-work'), (285, 302, 'creative-work')]}\n",
      "Spacy Prediction:\n",
      "[]\n",
      "-----------\n",
      "Stanford Prediction:\n",
      "[('Star Trek', ' creative-work'), ('Star Trek', ' creative-work'), ('Kirk', 'person'), ('the Making of Star Trek 1968', ' creative-work')]\n",
      "-----------\n",
      "Flair Prediction:\n",
      "[(88, 97, ' creative-work'), (148, 157, ' creative-work'), (181, 185, 'person'), (199, 222, ' creative-work')]\n",
      "-----------\n",
      "TagMe mentions:\n",
      "Tango [0,5] lp=0.2710080146789551\n",
      "section [15,22] lp=0.006431137211620808\n",
      "fictional [33,42] lp=0.026906762272119522\n",
      "enterprise [43,53] lp=0.04588697850704193\n",
      "Star Trek [88,97] lp=0.8303512334823608\n",
      "universe [98,106] lp=0.0762593150138855\n",
      "biographies [129,140] lp=0.02239430695772171\n",
      "characters [158,168] lp=0.034429747611284256\n",
      "Kirk [181,185] lp=0.05591510981321335\n",
      "copy of [191,198] lp=0.006864988710731268\n",
      "Making of [203,212] lp=0.011853307485580444\n",
      "packed [239,245] lp=0.0019941709470003843\n",
      "away [246,250] lp=0.0011786442482843995\n",
      "unreachable [255,266] lp=0.01515151560306549\n",
      "as is [267,272] lp=0.003583688521757722\n",
      "writers guide [289,302] lp=0.1764705926179886\n",
      "may [305,308] lp=0.0010793464025482535\n",
      "-----------\n",
      "TagMe annotations:\n",
      "Tango -> Tango (score: 0.13550400733947754)\n",
      "fictional -> Fiction (score: 0.1827511191368103)\n",
      "enterprise -> Enterprise (NX-01) (score: 0.26002347469329834)\n",
      "fictional -> Fictional universe (score: 0.26952943205833435)\n",
      "Star Trek -> Star Trek (score: 0.4151756167411804)\n",
      "Star Trek -> Star Trek (score: 0.6007955074310303)\n",
      "characters -> That's So Raven (score: 0.13648980855941772)\n",
      "Kirk -> James T. Kirk (score: 0.14723248779773712)\n",
      "Star Trek -> Star Trek (film) (score: 0.6000527739524841)\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from spacy_stanza import StanzaLanguage\n",
    "for text,entitites in pres_data: \n",
    "    print(text)\n",
    "    print(\"Correct entities:\")\n",
    "    print(entitites)\n",
    "    print(\"Spacy Prediction:\")\n",
    "    doc = nlp1(text)\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    print(predicted_entities)\n",
    "    print(\"-----------\")\n",
    "    print(\"Stanford Prediction:\")\n",
    "    entities = filter_entities(entitites)\n",
    "    nlp_stanford = StanzaLanguage(snlp)    \n",
    "    doc = nlp_stanford(text)\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    predicted_entities = [x for x in predicted_entities if x[1] in accepted_ents]\n",
    "    predicted_entities = [(x[0],stanf_to_conLL(x[1])) for x in predicted_entities ]\n",
    "    print(predicted_entities)\n",
    "    print(\"-----------\")\n",
    "    print(\"Flair Prediction:\")\n",
    "    entities = filter_entities(entities)\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    dict_ = (sentence.to_dict(tag_type='ner'))\n",
    "    predicted_entities = flair_to_spacy(dict_)\n",
    "    print(predicted_entities)\n",
    "    print(\"-----------\")\n",
    "    print(\"TagMe mentions:\")\n",
    "    tomatoes_mentions = tagme.mentions(text)\n",
    "    for mention in tomatoes_mentions.mentions:\n",
    "        print(mention)\n",
    "    print(\"-----------\")\n",
    "    print(\"TagMe annotations:\")\n",
    "    lunch_annotations = tagme.annotate(text)\n",
    "    for ann in lunch_annotations.get_annotations(0.1):\n",
    "        print(ann)\n",
    "    print(\"-----------\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-27 12:37:38,930 loading file /home/c3dric/.flair/models/en-ner-ontonotes-v0.4.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"I love Berlin .\"   [− Tokens: 4  − Token-Labels: \"I love Berlin <S-GPE> .\"]]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# make a sentence\n",
    "sentence = Sentence('I love Berlin .')\n",
    "\n",
    "# load the NER tagger\n",
    "tagger = SequenceTagger.load('ner-ontonotes')\n",
    "\n",
    "# run NER over sentence\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"I love Berlin .\"   [− Tokens: 4  − Token-Labels: \"I love Berlin <S-GPE> .\"]\n",
      "The following NER tags are found:\n",
      "Span [3]: \"Berlin\"   [− Labels: GPE (0.9763)]\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print('The following NER tags are found:')\n",
    "\n",
    "# iterate over entities and print\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.91 s, sys: 20.8 ms, total: 3.93 s\n",
      "Wall time: 2.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Sentence: \"& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .\"   [− Tokens: 34  − Token-Labels: \"& gt ; * Police last <B-DATE> week <E-DATE> evacuated 80 <S-CARDINAL> villagers from Waltengoo <B-GPE> Nar <E-GPE> where dozens <S-CARDINAL> were killed after a series of avalanches hit the area in 2005 <S-DATE> in the south of the territory .\"]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sentence = Sentence(EVAL_DATA[1][0])\n",
    "tagger.predict(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .',\n",
       " 'labels': [],\n",
       " 'entities': [{'text': 'last week',\n",
       "   'start_pos': 16,\n",
       "   'end_pos': 25,\n",
       "   'labels': [DATE (0.8501)]},\n",
       "  {'text': '80',\n",
       "   'start_pos': 36,\n",
       "   'end_pos': 38,\n",
       "   'labels': [CARDINAL (0.9999)]},\n",
       "  {'text': 'Waltengoo Nar',\n",
       "   'start_pos': 54,\n",
       "   'end_pos': 67,\n",
       "   'labels': [GPE (0.9608)]},\n",
       "  {'text': 'dozens',\n",
       "   'start_pos': 74,\n",
       "   'end_pos': 80,\n",
       "   'labels': [CARDINAL (0.9933)]},\n",
       "  {'text': '2005',\n",
       "   'start_pos': 138,\n",
       "   'end_pos': 142,\n",
       "   'labels': [DATE (0.9995)]}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = Sentence(EVAL_DATA[1][0])\n",
    "\n",
    "dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(33, 38, 'location')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_to_spacy(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flair_to_spacy(dict_):\n",
    "    res = []\n",
    "    entities = dict_['entities']\n",
    "    for ent in entities:\n",
    "        text = ent['text']\n",
    "        start_pos = ent['start_pos']\n",
    "        end_pos = ent['end_pos']        \n",
    "        label = ent['labels'][0].value\n",
    "        if (label in accepted_ents):\n",
    "            label = stanf_to_conLL(label)\n",
    "            res.append((start_pos,end_pos,label))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-27 13:01:13,844 loading file /home/c3dric/.flair/models/en-ner-ontonotes-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('ner-ontonotes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3639e9482f7448e1b39db007cb575292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1287.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FN: 706 FP: 296 TP: 307\n",
      "CPU times: user 57min 32s, sys: 11.7 s, total: 57min 44s\n",
      "Wall time: 31min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "fp_global = 0 \n",
    "fn_global = 0 \n",
    "tp_global = 0 \n",
    "stanford  = False\n",
    "flair = True\n",
    "accepted_ents = ['GPE', 'PERSON','ORG','FAC','WORK_OF_ART','NORP','PRODUCT']\n",
    "for text,entities in tqdm(EVAL_DATA):\n",
    "    fp = 0 \n",
    "    fn = 0 \n",
    "    tp = 0 \n",
    "\n",
    "    entities = filter_entities(entities)\n",
    "    sentence = Sentence(text)\n",
    "    tagger.predict(sentence)\n",
    "    dict_ = (sentence.to_dict(tag_type='ner'))\n",
    "    predicted_entities = flair_to_spacy(dict_)\n",
    "        \n",
    "    ent_as_list = list(entities.values())[0]\n",
    "    correct_entities = ent_as_list\n",
    "    correct_text = [text[y[0]:y[1]] for y in correct_entities]\n",
    "    correctly_predicted = 0 \n",
    "    correct_entities = set(correct_entities)\n",
    "    predicted_entities = set(predicted_entities)\n",
    "    tp = len(correct_entities.intersection(predicted_entities))\n",
    "    fp = len(predicted_entities - correct_entities)\n",
    "    fn = len(correct_entities - predicted_entities)\n",
    "    \n",
    "    fp_global += fp\n",
    "    fn_global += fn\n",
    "    tp_global += tp \n",
    "    \n",
    "    print(\"FN: \" + str(fn_global) + \" FP: \" + str(fp_global) + \" TP: \" + str(tp_global), end=\"\\r\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TagMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tagme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagme.GCUBE_TOKEN = \"edcf25a7-c492-49b8-b80e-bbee015fc687-843339462\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 8.42 ms, total: 23.2 ms\n",
      "Wall time: 843 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meal -> Meal (score: 0.2014230340719223)\n",
      "Mexican -> Mexican cuisine (score: 0.36614900827407837)\n",
      "burritos -> Burrito (score: 0.28607892990112305)\n"
     ]
    }
   ],
   "source": [
    "lunch_annotations = tagme.annotate(\"My favourite meal is Mexican burritos.\")\n",
    "for ann in lunch_annotations.get_annotations(0.1):\n",
    "    print(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ice cream [18,27] lp=0.18749085068702698\n",
      "tomatoes [40,48] lp=0.004235605709254742\n",
      "CPU times: user 18.5 ms, sys: 0 ns, total: 18.5 ms\n",
      "Wall time: 408 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tomatoes_mentions = tagme.mentions(\"I definitely like ice cream better than tomatoes.\")\n",
    "\n",
    "for mention in tomatoes_mentions.mentions:\n",
    "    print(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama and italy have a semantic relation of 0.53215491771698\n"
     ]
    }
   ],
   "source": [
    "# Get relatedness between a pair of entities specified by title.\n",
    "rels = tagme.relatedness_title((\"Barack Obama\", \"Donald Trump\"))\n",
    "print(\"Obama and italy have a semantic relation of\", rels.relatedness[0].rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b858a24f67364ed1b35815f28f88ea34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .\n",
      "['Sonmarg']\n",
      "gt -> Tonne (score: 0.11385387182235718)\n",
      "soldier -> United States Army (score: 0.12458904832601547)\n",
      "avalanche -> Allied invasion of Italy (score: 0.23905113339424133)\n",
      "army -> United States Army (score: 0.1549888253211975)\n",
      "army barracks -> Barracks (score: 0.23123230040073395)\n",
      "barracks -> Barracks (score: 0.2525090277194977)\n",
      "Sonmarg -> Sonamarg (score: 0.5)\n",
      "-----------------\n",
      "& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .\n",
      "['Waltengoo Nar']\n",
      "series of avalanches -> 2010 Salang avalanches (score: 0.25023046135902405)\n",
      "avalanches -> Avalanche (score: 0.2725068926811218)\n",
      "-----------------\n",
      "& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .\n",
      "[]\n",
      "Thursday -> Thursday (band) (score: 0.10039137303829193)\n",
      "who -> The Who (score: 0.17288707196712494)\n",
      "avalanche -> Avalanche (Matthew Good album) (score: 0.16760167479515076)\n",
      "-----------------\n",
      "& gt ; * The four civilians killed included two children of a family whose house was hit by a separate avalanche , also on Wednesday , a police spokesman said .\n",
      "[]\n",
      "gt -> Gross tonnage (score: 0.1088222861289978)\n",
      "civilians -> Civilian (score: 0.1804436445236206)\n",
      "killed -> Death of Osama bin Laden (score: 0.15027083456516266)\n",
      "children -> Angelina Jolie (score: 0.11307365447282791)\n",
      "family -> Family (score: 0.15441758930683136)\n",
      "house -> House (score: 0.11799762398004532)\n",
      "avalanche -> Colorado Avalanche (score: 0.10596161335706711)\n",
      "-----------------\n",
      "The bodies of the soldiers were recovered after the concerted efforts of the Avalanche Rescue Teams ( ART ) , which is equipped to work in inhospitable terrain and weather conditions .\n",
      "['Avalanche Rescue Teams', 'ART']\n",
      "Avalanche Rescue -> Avalanche rescue (score: 0.18518517911434174)\n",
      "ART -> Art (score: 0.24993830919265747)\n",
      "work -> Work of art (score: 0.1074819564819336)\n",
      "terrain -> Terrain (score: 0.10677191615104675)\n",
      "weather -> Weather (score: 0.15416544675827026)\n",
      "-----------------\n",
      "& gt ; * Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , Defence Spokesman Colonel Rajesh Kalia said .\n",
      "['Colonel Rajesh Kalia']\n",
      "mortal -> Death (score: 0.15444660186767578)\n",
      "martyrs -> Martyr (score: 0.15796484053134918)\n",
      "weather -> Weather (score: 0.10528197884559631)\n",
      "Defence -> Military (score: 0.11306709051132202)\n",
      "Colonel -> Colonel (score: 0.17764396965503693)\n",
      "-----------------\n",
      "Visuals of the avalanche site in Gurez sector .\n",
      "['Gurez sector']\n",
      "avalanche -> Avalanche (score: 0.1616418957710266)\n",
      "Gurez -> Gurais (score: 0.3125)\n",
      "-----------------\n",
      "( Source : ANI ) Visuals of the avalanche site in Gurez sector .\n",
      "['ANI', 'Gurez sector']\n",
      "avalanche -> Avalanche (score: 0.13170114159584045)\n",
      "Gurez -> Gurais (score: 0.33970677852630615)\n",
      "-----------------\n",
      "( Source : ANI )\n",
      "['ANI']\n",
      "Source -> Source code (score: 0.24645555019378662)\n",
      "ANI -> ANI (file format) (score: 0.3079666495323181)\n",
      "-----------------\n",
      "“ Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , ” Defence Spokesman Colonel Rajesh Kalia said .\n",
      "['Rajesh Kalia']\n",
      "mortal -> Death (score: 0.15444660186767578)\n",
      "martyrs -> Martyr (score: 0.15796484053134918)\n",
      "weather -> Weather (score: 0.15911443531513214)\n",
      "Defence -> Military (score: 0.11306709051132202)\n",
      "Colonel -> Colonel (score: 0.17764396965503693)\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text,entities in tqdm(EVAL_DATA[0:10]):\n",
    "    ent_as_list = list(entities.values())[0]\n",
    "    correct_entities = ent_as_list\n",
    "    correct_text = [text[y[0]:y[1]] for y in correct_entities]\n",
    "    lunch_annotations = tagme.annotate(text)\n",
    "    print(text)\n",
    "    print(correct_text)\n",
    "    for ann in lunch_annotations.get_annotations(0.1):\n",
    "        print(ann)\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tomatoes_mentions = tagme.mentions(\"I definitely like ice cream better than tomatoes.\")\n",
    "\n",
    "for mention in tomatoes_mentions.mentions:\n",
    "    print(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd3ca67b124828991e8ec912713129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .\n",
      "['Sonmarg']\n",
      "2msec, 12 mentions\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "soldier [13,20] lp=0.03945669159293175\n",
      "killed [25,31] lp=0.002261004177853465\n",
      "avalanche [45,54] lp=0.21192322671413422\n",
      "hit [55,58] lp=0.01779121533036232\n",
      "army [62,66] lp=0.029292907565832138\n",
      "army barracks [62,75] lp=0.023383768275380135\n",
      "barracks [67,75] lp=0.06593720614910126\n",
      "northern area [83,96] lp=0.011247443966567516\n",
      "Sonmarg [100,107] lp=1.0\n",
      "military [117,125] lp=0.022062312811613083\n",
      "spokesman [126,135] lp=0.014206210151314735\n",
      "---------------\n",
      "& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .\n",
      "['Waltengoo Nar']\n",
      "2msec, 16 mentions\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "Police [9,15] lp=0.03883015364408493\n",
      "last [16,20] lp=0.002006721217185259\n",
      "last week [16,25] lp=0.0016004267381504178\n",
      "week [21,25] lp=0.0021859135013073683\n",
      "evacuated [26,35] lp=0.01497990544885397\n",
      "villagers [39,48] lp=0.010473696514964104\n",
      "Nar [64,67] lp=0.056971512734889984\n",
      "dozens [74,80] lp=0.002762286923825741\n",
      "killed [86,92] lp=0.002261004177853465\n",
      "series of avalanches [101,121] lp=0.1818181872367859\n",
      "avalanches [111,121] lp=0.22637106478214264\n",
      "hit [122,125] lp=0.01779121533036232\n",
      "area [130,134] lp=0.006615964230149984\n",
      "south [150,155] lp=0.005982078146189451\n",
      "territory [163,172] lp=0.004919565748423338\n",
      "---------------\n",
      "& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .\n",
      "[]\n",
      "2msec, 9 mentions\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "army [13,17] lp=0.029292907565832138\n",
      "Thursday [21,29] lp=0.018874140456318855\n",
      "bodies [44,50] lp=0.0037941946648061275\n",
      "ten [54,57] lp=0.003412862541154027\n",
      "men [65,68] lp=0.004728082101792097\n",
      "who [69,72] lp=0.003441077657043934\n",
      "killed [78,84] lp=0.002261004177853465\n",
      "avalanche [91,100] lp=0.21192322671413422\n",
      "---------------\n",
      "& gt ; * The four civilians killed included two children of a family whose house was hit by a separate avalanche , also on Wednesday , a police spokesman said .\n",
      "[]\n",
      "2msec, 11 mentions\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "civilians [18,27] lp=0.014031961560249329\n",
      "killed [28,34] lp=0.002261004177853465\n",
      "children [48,56] lp=0.004043775610625744\n",
      "family [62,68] lp=0.05819566175341606\n",
      "house [75,80] lp=0.01210733875632286\n",
      "hit [85,88] lp=0.01779121533036232\n",
      "avalanche [103,112] lp=0.21192322671413422\n",
      "Wednesday [123,132] lp=0.027359316125512123\n",
      "police [137,143] lp=0.03883015364408493\n",
      "spokesman [144,153] lp=0.014206210151314735\n",
      "---------------\n",
      "The bodies of the soldiers were recovered after the concerted efforts of the Avalanche Rescue Teams ( ART ) , which is equipped to work in inhospitable terrain and weather conditions .\n",
      "['Avalanche Rescue Teams', 'ART']\n",
      "2msec, 10 mentions\n",
      "bodies [4,10] lp=0.0037941946648061275\n",
      "soldiers [18,26] lp=0.006199108436703682\n",
      "concerted [52,61] lp=0.00996441300958395\n",
      "Avalanche Rescue [77,93] lp=0.37037035822868347\n",
      "Teams [94,99] lp=0.0011753265280276537\n",
      "ART [102,105] lp=0.01620296761393547\n",
      "work [131,135] lp=0.001639241585507989\n",
      "inhospitable [139,151] lp=0.0017621145816519856\n",
      "terrain [152,159] lp=0.019628461450338364\n",
      "weather [164,171] lp=0.017457839101552963\n",
      "---------------\n",
      "& gt ; * Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , Defence Spokesman Colonel Rajesh Kalia said .\n",
      "['Colonel Rajesh Kalia']\n",
      "1msec, 13 mentions\n",
      "gt [2,4] lp=0.034230880439281464\n",
      "Arrangements [9,21] lp=0.005740774795413017\n",
      "carry [38,43] lp=0.007117811590433121\n",
      "mortal [48,54] lp=0.01314708311110735\n",
      "martyrs [70,77] lp=0.029219815507531166\n",
      "native [87,93] lp=0.013165488839149475\n",
      "weather [119,126] lp=0.017457839101552963\n",
      "clear [135,140] lp=0.0027307576965540648\n",
      "Defence [143,150] lp=0.011256588622927666\n",
      "Spokesman [151,160] lp=0.014206210151314735\n",
      "Colonel [161,168] lp=0.18073730170726776\n",
      "Rajesh [169,175] lp=0.08641444891691208\n",
      "Kalia [176,181] lp=0.15744680166244507\n",
      "---------------\n",
      "Visuals of the avalanche site in Gurez sector .\n",
      "['Gurez sector']\n",
      "0msec, 4 mentions\n",
      "Visuals [0,7] lp=0.004607283975929022\n",
      "avalanche [15,24] lp=0.21192322671413422\n",
      "Gurez [33,38] lp=0.625\n",
      "sector [39,45] lp=0.0033981569577008486\n",
      "---------------\n",
      "( Source : ANI ) Visuals of the avalanche site in Gurez sector .\n",
      "['ANI', 'Gurez sector']\n",
      "0msec, 6 mentions\n",
      "Source [2,8] lp=0.010200803168118\n",
      "ANI [11,14] lp=0.13322299718856812\n",
      "Visuals [17,24] lp=0.004607283975929022\n",
      "avalanche [32,41] lp=0.21192322671413422\n",
      "Gurez [50,55] lp=0.625\n",
      "sector [56,62] lp=0.0033981569577008486\n",
      "---------------\n",
      "( Source : ANI )\n",
      "['ANI']\n",
      "0msec, 2 mentions\n",
      "Source [2,8] lp=0.010200803168118\n",
      "ANI [11,14] lp=0.13322299718856812\n",
      "---------------\n",
      "“ Arrangements are in place to carry the mortal remains of the martyrs to their native places immediately after weather becomes clear , ” Defence Spokesman Colonel Rajesh Kalia said .\n",
      "['Rajesh Kalia']\n",
      "2msec, 12 mentions\n",
      "Arrangements [2,14] lp=0.005740774795413017\n",
      "carry [31,36] lp=0.007117811590433121\n",
      "mortal [41,47] lp=0.01314708311110735\n",
      "martyrs [63,70] lp=0.029219815507531166\n",
      "native [80,86] lp=0.013165488839149475\n",
      "weather [112,119] lp=0.017457839101552963\n",
      "clear [128,133] lp=0.0027307576965540648\n",
      "Defence [138,145] lp=0.011256588622927666\n",
      "Spokesman [146,155] lp=0.014206210151314735\n",
      "Colonel [156,163] lp=0.18073730170726776\n",
      "Rajesh [164,170] lp=0.08641444891691208\n",
      "Kalia [171,176] lp=0.15744680166244507\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text,entities in tqdm(EVAL_DATA[0:10]):\n",
    "    ent_as_list = list(entities.values())[0]\n",
    "    correct_entities = ent_as_list\n",
    "    correct_text = [text[y[0]:y[1]] for y in correct_entities]\n",
    "    lunch_annotations = tagme.annotate(text)\n",
    "    print(text)\n",
    "    print(correct_text)\n",
    "    tomatoes_mentions = tagme.mentions(text)\n",
    "    print(tomatoes_mentions)\n",
    "    for mention in tomatoes_mentions.mentions:\n",
    "        print(mention)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
