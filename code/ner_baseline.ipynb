{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path) as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut_path = \"../data/wnut17train.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/wnut17train.conll\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i,row in df.iterrows():\n",
    "    if(row.O != 'O' and (not isinstance(row.O,float))):\n",
    "        labels.append(row.O)\n",
    "labels = [x[2:len(x)] for x in labels]\n",
    "labels = set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corporation', 'creative-work', 'group', 'location', 'person', 'product'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['@paulwalk', 'O'], ['It', 'O'], [\"'s\", 'O'], ['the', 'O'], ['view', 'O']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [x.replace(\"\\t\", \" \") for x in read_file(wnut_path)]\n",
    "text = [x.split() for x in content]\n",
    "text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(list_labels):\n",
    "    res = []\n",
    "    for i,x in enumerate(list_labels):\n",
    "        j = 1\n",
    "        if(i + j< len(list_labels) and x[1] == 'B' and list_labels[i  + j][1] == 'I'):\n",
    "            while(i + j < len(list_labels) and list_labels[i + j][1] == 'I'):\n",
    "                j += 1\n",
    "            res.append((x[0],x[0] + j -1 ,x[2]))\n",
    "        elif(x[1] == 'B'):\n",
    "            res.append([x[0],x[2]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"This is a test\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(x) for x in test[0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_i_to_char_i(sent_label):\n",
    "    res_labels = []\n",
    "    sent = sent_label[0].split()\n",
    "    labels = sent_label[1]\n",
    "    \n",
    "    for label in labels: \n",
    "        if(len(label) == 2):\n",
    "            start_label_index = label[0]\n",
    "            end_label_index = label[0]\n",
    "            label_name = label[1]\n",
    "        elif(len(label) == 3):\n",
    "            start_label_index = label[0]\n",
    "            end_label_index = label[1]\n",
    "            label_name = label[2]\n",
    "            \n",
    "\n",
    "\n",
    "        start_char_index = sum([len(x) for x in sent[0:start_label_index]])    \n",
    "        end_char_index = start_char_index + sum([len(x) for x in sent[start_label_index:end_label_index + 1]])\n",
    "        res_labels.append((start_char_index + start_label_index,end_char_index + end_label_index,label_name))\n",
    "        \n",
    "            \n",
    "    return [\" \".join(sent), res_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_spacy_format(sent_label): \n",
    "    sent = sent_label[0]\n",
    "    labels = sent_label[1]\n",
    "    res_dict = {'entities': labels}\n",
    "    return [sent, res_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "res = []\n",
    "sentence = []\n",
    "sent_labels = []\n",
    "for x in text:\n",
    "    if(len(x)!=0):\n",
    "        sentence.append(x[0])\n",
    "        sent_labels.append(x[1])\n",
    "    else:\n",
    "        sent_labels = [(i,x[0], x[2:len(x)]) for i,x in enumerate(sent_labels) if x != 'O' ]\n",
    "        sent_labels = squash(sent_labels)\n",
    "        res.append([\" \".join(sentence),sent_labels])\n",
    "        sent_labels = []\n",
    "        sentence = []\n",
    "        \n",
    "res = [str_i_to_char_i(x) for x in res]\n",
    "res = [to_spacy_format(x) for x in res]\n",
    "TRAIN_DATA = res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "Add ner pipe\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import spacy\n",
    "model = None\n",
    "n_iter=10\n",
    "\n",
    "if model is not None:\n",
    "    nlp1 = spacy.load(model)  # load existing spaCy model\n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp1 = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "#create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp1.pipe_names:\n",
    "    print(\"Add ner pipe\")\n",
    "    ner = nlp1.create_pipe('ner')\n",
    "    nlp1.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp1.get_pipe('ner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =\"~~hey guys, i got this switch up for grabs, the website is my switch patched shows that it is hackable, but i believe the latest update has been applied so may not be hackable anymore? i dont really know.\\n\\nI had given this to my son last xmass but then he asked for a laptop so in part payment he gave me the switch. under his possession he always had a screen protector in it. It does however have like 3 or 4 small scratches that are not visible while playing. The body overall has normal signs of use like light scratches and what not but nothing crazy. The games included are Smash Ultimate, Mariokart 8 Deluxe and Minecraft. It does have a new pair of joy cons because the old left one the bumper is bad, maybe an easy fix, but didn't bother and just bought another set. The old are included too. The battery seems to be holding a charge just fine. I just dont use it, rather play games on my pc as well.\\n\\nI do have the original box, and what i think is most of the accessories check pics for all the is included. \\n\\npictures: https://imgur.com/a/Rr3jRoJ\\n\\nLooking for 280  locally (HOUSTON- GALVESTON area) \\n\\nshipped looking for about 300 to cover shipping.  Rather sell locally so sort of firm on the shipped price.~~\\n\\nSOLD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (x, {\n",
    "        'entities': [(23, 29, 'PRODUCT'), (62, 68, 'PRODUCT'), (309, 315, 'PRODUCT'), (411, 426, 'CONDITION'), (509, 524, 'CONDITION'), (1031, 1067, 'URL'),\n",
    "                    (1086, 1109, 'LOCATION'), (1072, 1084, 'LOCALPRICE')]\n",
    "    })]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, Trains data based on annotations \n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3394/3394 [01:49<00:00, 30.99it/s]\n",
      "  0%|          | 4/3394 [00:00<01:54, 29.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4750.463819609098}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3394/3394 [01:54<00:00, 29.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4508.24149024284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp1.pipe_names if pipe != 'ner']\n",
    "\n",
    "with nlp1.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp1.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(TRAIN_DATA):\n",
    "            nlp1.update(\n",
    "                [text],  # batch of texts\n",
    "                [annotations],  # batch of annotations\n",
    "                drop=0.5,  # dropout \n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2fbe34f553e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRAIN_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Entities'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             )\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "# test the trained model\n",
    "for text, _ in TRAIN_DATA:\n",
    "    doc = nlp1(x)\n",
    "    print('Entities', [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
